{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44297df1-1c95-4c97-9589-434c2a8f051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.cluster import LatentDirichletAllocation\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_features, annotations):\n",
    "        self.video_features = video_features\n",
    "        self.annotations = annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_feature = self.video_features[idx]\n",
    "        annotation = self.annotations[idx]\n",
    "        return video_feature, annotation\n",
    "\n",
    "def extract_frame_features(video_path):\n",
    "    \"\"\"Extract frame-level features using a pre-trained model.\"\"\"\n",
    "    # Placeholder for pre-trained feature extraction (e.g., C3D, TSN)\n",
    "    pass\n",
    "\n",
    "def reduce_dimensions(features, n_components=500):\n",
    "    \"\"\"Reduce feature dimensions using PCA.\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(features)\n",
    "\n",
    "# 2. Temporal Event Proposal (TEP)\n",
    "class TemporalEventProposal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_proposals):\n",
    "        super(TemporalEventProposal, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_proposals)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, _ = self.gru(x)\n",
    "        proposal_scores = self.sigmoid(self.fc(outputs))\n",
    "        return proposal_scores\n",
    "\n",
    "# 3. Hierarchical Representation\n",
    "class TemporalSemanticRelationModule(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TemporalSemanticRelationModule, self).__init__()\n",
    "        self.fc_temporal = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(output_dim, 1)\n",
    "        )\n",
    "        self.fc_semantic = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, proposals, context):\n",
    "        temporal_scores = self.fc_temporal(proposals)\n",
    "        semantic_scores = torch.matmul(self.fc_semantic(proposals), self.fc_semantic(context).T)\n",
    "        final_scores = self.softmax(temporal_scores * semantic_scores)\n",
    "        return final_scores\n",
    "\n",
    "class TopicPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, topic_dim):\n",
    "        super(TopicPredictor, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_dim, input_dim, kernel_size=3, stride=2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, topic_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(topic_dim, topic_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.permute(0, 2, 1))  # (batch, features, frames)\n",
    "        x = torch.max(x, dim=-1)[0]  # Max-pooling\n",
    "        return self.fc(x)\n",
    "\n",
    "# 4. Caption Generator\n",
    "class CaptionGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CaptionGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, captions, features):\n",
    "        embedded = self.embedding(captions)\n",
    "        lstm_input = torch.cat((features.unsqueeze(1), embedded), dim=1)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        return self.fc(lstm_out)\n",
    "\n",
    "# 5. Temporal-Linguistic NMS\n",
    "def temporal_linguistic_nms(proposals, captions, scores, iou_threshold=0.5, similarity_threshold=0.5):\n",
    "    \"\"\"Remove duplicate proposals and captions.\"\"\"\n",
    "    selected = []\n",
    "    while proposals:\n",
    "        best_idx = scores.argmax()\n",
    "        selected.append((proposals[best_idx], captions[best_idx]))\n",
    "        proposals.pop(best_idx)\n",
    "        captions.pop(best_idx)\n",
    "        scores.pop(best_idx)\n",
    "\n",
    "        # Filter proposals based on thresholds (placeholder for actual logic)\n",
    "        proposals = [p for i, p in enumerate(proposals) if i != best_idx]\n",
    "\n",
    "    return selected\n",
    "\n",
    "# 6. Save and Load Model\n",
    "def save_model(model, path):\n",
    "    \"\"\"Save the trained model to a file.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model_class, path, *args, **kwargs):\n",
    "    \"\"\"Load a model from a file.\"\"\"\n",
    "    model = model_class(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 7. Inference Function\n",
    "def generate_dense_caption(video_features, feature_extractor, tsrm, topic_predictor, caption_generator):\n",
    "    \"\"\"Generate dense video captions for given video features.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Extract proposals\n",
    "        proposals = feature_extractor(video_features)\n",
    "\n",
    "        # Hierarchical representation\n",
    "        event_relations = tsrm(proposals, proposals)\n",
    "        topics = topic_predictor(video_features)\n",
    "\n",
    "        # Generate captions\n",
    "        generated_captions = caption_generator(torch.zeros((1, 10), dtype=torch.long), event_relations)\n",
    "        return generated_captions\n",
    "\n",
    "# 8. Training Loop\n",
    "def train_model(feature_extractor, tsrm, topic_predictor, caption_generator, dataloader, num_epochs, save_path):\n",
    "    optimizer = optim.Adam(list(feature_extractor.parameters()) +\n",
    "                            list(tsrm.parameters()) +\n",
    "                            list(topic_predictor.parameters()) +\n",
    "                            list(caption_generator.parameters()), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for video_features, annotations in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Extract Proposals\n",
    "            proposals = feature_extractor(video_features)\n",
    "\n",
    "            # Hierarchical Representation\n",
    "            event_relations = tsrm(proposals, proposals)\n",
    "            topics = topic_predictor(video_features)\n",
    "\n",
    "            # Generate Captions\n",
    "            captions = [ann[\"caption\"] for ann in annotations]\n",
    "            targets = torch.tensor([ann[\"target\"] for ann in annotations])\n",
    "            outputs = caption_generator(torch.tensor(captions), event_relations)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the final model\n",
    "    save_model(feature_extractor, os.path.join(save_path, \"feature_extractor.pth\"))\n",
    "    save_model(tsrm, os.path.join(save_path, \"tsrm.pth\"))\n",
    "    save_model(topic_predictor, os.path.join(save_path, \"topic_predictor.pth\"))\n",
    "    save_model(caption_generator, os.path.join(save_path, \"caption_generator.pth\"))\n",
    "\n",
    "# Instantiate and Train\n",
    "if __name__ == \"__main__\":\n",
    "    # Placeholder: Load dataset and annotations\n",
    "    video_features = []  # Replace with actual feature loading logic\n",
    "    annotations = []  # Replace with actual annotations loading logic\n",
    "\n",
    "    dataset = VideoDataset(video_features, annotations)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Model components\n",
    "    feature_extractor = TemporalEventProposal(input_dim=500, hidden_dim=512, num_proposals=10)\n",
    "    tsrm = TemporalSemanticRelationModule(input_dim=512, output_dim=512)\n",
    "    topic_predictor = TopicPredictor(input_dim=512, topic_dim=100)\n",
    "    caption_generator = CaptionGenerator(vocab_size=10000, embed_dim=300, hidden_dim=512)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(feature_extractor, tsrm, topic_predictor, caption_generator, dataloader, num_epochs=10, save_path=\"./models\")\n",
    "\n",
    "    # Example: Load and run inference\n",
    "    feature_extractor = load_model(TemporalEventProposal, \"./models/feature_extractor.pth\", input_dim=500, hidden_dim=512, num_proposals=10)\n",
    "    tsrm = load_model(TemporalSemanticRelationModule, \"./models/tsrm.pth\", input_dim=512, output_dim=512)\n",
    "    topic_predictor = load_model(TopicPredictor, \"./models/topic_predictor.pth\", input_dim=512, topic_dim=100)\n",
    "    caption_generator = load_model(CaptionGenerator, \"./models/caption_generator.pth\", vocab_size=10000, embed_dim=300, hidden_dim=512)\n",
    "\n",
    "    # Inference example\n",
    "    test_video_features = torch.randn(1, 500)  # Replace with actual test video features\n",
    "    captions = generate_dense_caption(test_video_features, feature_extractor, tsrm, topic_predictor, caption_generator)\n",
    "    print(\"Generated Captions:\", captions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
